---
layout: post
title: "A thought about improving AI"
date: 2018-07-23
tags: Philosophy AI
---

When it comes to AI, it is almost always said that it is still being improved. Of course, there are usually reasons given (and usually implicitly) why it has to be improved – for better security, safety or usability.

But, a few days ago a thought came to my mind: what does "improved" exactly mean? As it is never explained in the articles, one can assume that it is not confusing. I asked myself: is it really?

**Here, please remember: the thoughts below are my own opinions. I am a layman and I have no relationship to AI in my everyday life.**

Since AI means intelligence that is artificial, I understand this at first as "made" (in contrary to "natural"). But still, it is not clear, at least for me, what does it mean that something that is made "can be improved" or "is being improved", or "is improved".

Let us try to understand this.

Firstly, you cannot just "improve". You have to improve something (sometimes of course it is presumed implicitly) – that is how the logic and the language work. For example, if you say "my english has improved", then there is implicit assumption that it improved itself.

Then, "to improve a thing" means "to make a thing better". In this statement there is yet one word to explain: "better".

It is always presumed that "better" means "as good as something else" (but often implicitly). But there, an obvious question comes to mind: what does it mean "to be good"?

I have answered this myself and this is my definition:

_When a thing made by someone (the creator) is good, it means that it meets the requirements that its creator is expected to make his thing to meet, or expects himself the thing to meet._

This translates the whole statement "to improve a thing" to "to make a thing meet the requirements (that its createor... and the rest of the above definition)". 

So now, our thing has to meet some requirements. As this is, I presume, clear, we can make further distinction:

1. Who is the creator of the thing == who is responsible for meeting the requirements by the thing?
2. What are the requirements?

And when it comes to AI, the answer for the first question is:

1. We, humans, are the creators (at least as it is nowadays).

The answer for the second one would be more complicated (if not impossible, as there are many requirements, not all are written and they are changing). But as I see it, the requirements can be divided into two main groups:

- the requirements about us – which may be understood as: the AI has to be as good as is our intelligence;
- the requirements about everything – which may be understood as: the AI has to be as good as it can be.

There are of course other choices ("as good as an earlier version of itself", "as good as something another else" etc.). But, I see these two the most frequently in articles over the internet – so I will stick to them in this article.

These two cases may indeed be separate if we presume that we, humans, are not "as good as we can be". Tto simplify the rest of my thoughts, I will be presuming this state across the rest of the article (and hoping that you all agree with me).

So, let us go back to the AI: is that enough that we know that there are some requirements which must be met, so that we can improve AI?

I think that is possibly not enough – specifically then, when we want to make it clear (for ourselves) what we are doing. Maybe some of the requirements are harmful or will have harmful effects for some of us? Since it is crucial for ourselves that we know what we are doing to ourselves, let us get back.

According to my definition (written above), the creator – we – either is expected, or expects. Since there is nobody else who can expect (at least nowadays), only the second statement is true.

So, we expect the AI to meet some requirements, implicitly understanding: which are provided by us. But still, there is unclear what these are: do they belong to the aformentioned first group (the requirements about us) or to the second (the requirements about everything)?

The opinions are shared, at least as I can assume from what I have read. Let us go back to the begginning of this article: I found that it is almost always implicit what the author of an article is presuming about the requirements.

For me, this is bad if not wrong. If we know that there is something unclear about a thing, then we cannot rely on that thing. And when we know that a thing is unclear from the definition (since intelligence, I presume, still is), it is not wise to make the definition itself unclear as well (to enhance the unclarity).

Therefore, to make clear what group the requirements belong to, we may answer many different questions. But, I propose to start from the following: are they adequate to the context?

By "context", I mean the following five things:
1. "What" – who is required (the subject)?
2. "Who" – who is requiring (the object)?
3. "When" – in what situation will a requirement be effective?
4. "Where" – in what domain will a requirement be effective? (By "a domain" I mean also: a field, a discipline.)
5. "What for" – what a requirement should be met for?

By "adequate", I mean the following five things:
1. The subject is objectively (that is, not just in someone's opinion) not against the requirement in the current context (that is, the rest of these points).
2. The object can require in the current context.
3. The situation cannot (not just: does not) have two requirements that are in contradiction in the current context.
4. The domain cannot (not just: does not) have two requirements that are in contradiction in the current context.
5. One can give at least one reason what is a requirement for in the current context.

Taking all that ten points and having them answered about AI, we can (probably) say with just a little doubt what does it mean for us "to improve AI". But: it is a field of consideration how to answer these questions. Maybe a good subject for a future article?